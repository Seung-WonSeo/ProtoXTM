{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a43bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dcf14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, num_topic, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc11 = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.fc12 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, num_topic)\n",
    "        self.fc22 = nn.Linear(hidden_dim, num_topic)\n",
    "\n",
    "        self.fc1_drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.mean_bn = nn.BatchNorm1d(num_topic, affine=True)\n",
    "        self.mean_bn.weight.requires_grad = False\n",
    "        self.logvar_bn = nn.BatchNorm1d(num_topic, affine=True)\n",
    "        self.logvar_bn.weight.requires_grad = False\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + (eps * std)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = F.softplus(self.fc11(x))\n",
    "        e1 = F.softplus(self.fc12(e1))\n",
    "        e1 = self.fc1_drop(e1)\n",
    "        mu = self.mean_bn(self.fc21(e1))\n",
    "        logvar = self.logvar_bn(self.fc22(e1))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0debc8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPS_XTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, vocab_size_en, vocab_size_cn,\n",
    "                 num_topics, DCL_weight, temperature, en_units=200, dropout=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.DCL_weight = DCL_weight\n",
    "        self.num_topics = num_topics\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.encoder = MLPEncoder(input_size, num_topics, en_units, dropout)\n",
    "        # self.encoder_en = MLPEncoder(input_size, num_topics, en_units, dropout)\n",
    "        # self.encoder_cn = MLPEncoder(input_size, num_topics, en_units, dropout)\n",
    "        self.z_drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.a = 1 * np.ones((1, int(num_topics))).astype(np.float32)\n",
    "        self.mu2 = nn.Parameter(torch.as_tensor((np.log(self.a).T - np.mean(np.log(self.a), 1)).T), requires_grad=False)\n",
    "        self.var2 = nn.Parameter(torch.as_tensor((((1.0 / self.a) * (1 - (2.0 / num_topics))).T + (1.0 / (num_topics * num_topics)) * np.sum(1.0 / self.a, 1)).T), requires_grad=False)\n",
    "\n",
    "        self.decoder_bn_en = nn.BatchNorm1d(vocab_size_en, affine=True)\n",
    "        self.decoder_bn_en.weight.requires_grad = False\n",
    "        self.decoder_bn_cn = nn.BatchNorm1d(vocab_size_cn, affine=True)\n",
    "        self.decoder_bn_cn.weight.requires_grad = False\n",
    "\n",
    "        self.phi_en = nn.Parameter(nn.init.xavier_uniform_(torch.empty((num_topics, vocab_size_en))))\n",
    "        self.phi_cn = nn.Parameter(nn.init.xavier_uniform_(torch.empty((num_topics, vocab_size_en))))\n",
    "        \n",
    "    def get_beta(self):\n",
    "        beta_en = self.phi_en\n",
    "        beta_cn = self.phi_cn\n",
    "        return beta_en, beta_cn\n",
    "    \n",
    "    \n",
    "    def get_latent_vector(self, x):\n",
    "        \n",
    "        z, mu, logvar = self.encoder(x)\n",
    "\n",
    "        if self.training:\n",
    "            return z, mu, logvar\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    def get_latent_vector_en(self, x):\n",
    "        z, mu, logvar = self.encoder_en(x)\n",
    "\n",
    "        if self.training:\n",
    "            return z, mu, logvar\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def get_latent_vector_cn(self, x):\n",
    "        z, mu, logvar = self.encoder_cn(x)\n",
    "\n",
    "        if self.training:\n",
    "            return z, mu, logvar\n",
    "        else:\n",
    "            return mu\n",
    "    '''\n",
    "        \n",
    "    def get_theta(self, x):\n",
    "        theta = F.softmax(x, dim=1)\n",
    "        theta = self.z_drop(theta)\n",
    "        return theta\n",
    "\n",
    "    def decode(self, theta, beta, lang):\n",
    "        bn = getattr(self, f'decoder_bn_{lang}')\n",
    "        d1 = F.softmax(bn(torch.matmul(theta, beta)), dim=1)\n",
    "        return d1\n",
    "    \n",
    "    def forward(self, x_en, x_cn, x_en_bow, x_cn_bow, labels_en, labels_cn, labels_c2e, labels_e2c):\n",
    "                \n",
    "        z_en, mu_en, logvar_en = self.get_latent_vector(x_en)\n",
    "        z_cn, mu_cn, logvar_cn = self.get_latent_vector(x_cn)\n",
    "        \n",
    "        dcl_loss = 0.\n",
    "        \n",
    "        dcl_loss_e2c = self.compute_dcl_loss(z_en, z_cn, labels_en, labels_e2c)\n",
    "        dcl_loss_c2e = self.compute_dcl_loss(z_cn, z_en, labels_cn, labels_c2e)\n",
    "        \n",
    "        dcl_loss = dcl_loss_e2c + dcl_loss_c2e\n",
    "        # dcl_loss = dcl_loss_e2c\n",
    "        # dcl_loss = dcl_loss_c2e\n",
    "        dcl_loss = self.DCL_weight * dcl_loss\n",
    "        \n",
    "        theta_en = self.get_theta(z_en)\n",
    "        theta_cn = self.get_theta(z_cn)\n",
    "\n",
    "        beta_en, beta_cn = self.get_beta()\n",
    "\n",
    "        TM_loss = 0.\n",
    "\n",
    "        x_recon_en = self.decode(theta_en, beta_en, lang='en')\n",
    "        x_recon_cn = self.decode(theta_cn, beta_cn, lang='cn')\n",
    "        loss_en = self.compute_loss_TM(x_recon_en, x_en_bow, mu_en, logvar_en)\n",
    "        loss_cn = self.compute_loss_TM(x_recon_cn, x_cn_bow, mu_cn, logvar_cn)\n",
    "\n",
    "        TM_loss = loss_en + loss_cn\n",
    "\n",
    "        total_loss = TM_loss + dcl_loss\n",
    "        \n",
    "        rst_dict = {\n",
    "            'topic_modeling_loss': TM_loss,\n",
    "            'contrastive_loss': dcl_loss,\n",
    "            'total_loss': total_loss\n",
    "        }\n",
    "\n",
    "        return rst_dict\n",
    "\n",
    "    def compute_loss_TM(self, recon_x, x, mu, logvar):\n",
    "        var = logvar.exp()\n",
    "        var_division = var / self.var2\n",
    "        diff = mu - self.mu2\n",
    "        diff_term = diff * diff / self.var2\n",
    "        logvar_division = self.var2.log() - logvar\n",
    "        KLD = 0.5 * ((var_division + diff_term + logvar_division).sum(1) - self.num_topics)\n",
    "\n",
    "        RECON = -(x * (recon_x + 1e-10).log()).sum(1)\n",
    "\n",
    "        LOSS = (RECON + KLD).mean()\n",
    "        return LOSS\n",
    "    \n",
    "    \n",
    "    def compute_dcl_loss(self, z_en, z_cn, labels_en, labels_e2c):\n",
    "        batch_size, embedding_dim = z_en.size()\n",
    "\n",
    "        # Initialize prototypes for each label\n",
    "        unique_labels = torch.unique(torch.cat((labels_en, labels_e2c)))\n",
    "        prototypes_en = torch.zeros((len(unique_labels), embedding_dim), device=z_en.device)\n",
    "        prototypes_cn = torch.zeros((len(unique_labels), embedding_dim), device=z_cn.device)\n",
    "\n",
    "        # Compute prototypes for English and Chinese embeddings\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            en_mask = (labels_en == label).unsqueeze(-1)  # Mask for English documents with label\n",
    "            cn_mask = (labels_e2c == label).unsqueeze(-1)  # Mask for Chinese documents with label\n",
    "\n",
    "            if en_mask.any():\n",
    "                prototypes_en[i] = (z_en * en_mask).sum(dim=0) / (en_mask.sum() + 1e-8)  # Avoid division by zero\n",
    "            if cn_mask.any():\n",
    "                prototypes_cn[i] = (z_cn * cn_mask).sum(dim=0) / (cn_mask.sum() + 1e-8)  # Avoid division by zero\n",
    "\n",
    "        # Compute anchor-positive similarities\n",
    "        logits = torch.mm(prototypes_en, prototypes_cn.t())  # Similarity matrix between English and Chinese prototypes\n",
    "        logits /= self.temperature  # Apply temperature scaling\n",
    "\n",
    "        # Normalize prototypes\n",
    "        logits /= torch.norm(prototypes_en, dim=1, keepdim=True) + 1e-8  # Avoid division by zero\n",
    "        logits /= (torch.norm(prototypes_cn, dim=1, keepdim=True).t() + 1e-8)  # Avoid division by zero\n",
    "\n",
    "        # Create positive mask\n",
    "        positive_mask = torch.eye(len(unique_labels), device=z_en.device)\n",
    "\n",
    "        # Compute InfoNCE loss\n",
    "        numerator = torch.exp(logits) * positive_mask\n",
    "        denominator = torch.exp(torch.clamp(logits, min=-1e4, max=1e4))  # Clip logits to avoid large values\n",
    "\n",
    "        # Avoid divide-by-zero\n",
    "        loss = -torch.log((numerator.sum(dim=1) + 1e-8) / (denominator.sum(dim=1) + 1e-8))\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d8876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "\n",
    "def create_dataloader_separate(sbert_doc_embeddings_en, bow_en, labels_en, labels_c2e,\n",
    "                               sbert_doc_embeddings_cn, bow_cn, labels_cn, labels_e2c,\n",
    "                               batch_size_en, batch_size_cn):\n",
    "    # Convert English data to tensors\n",
    "    embeddings_en = torch.tensor(sbert_doc_embeddings_en, dtype=torch.float32)\n",
    "    bow_en_tensor = torch.tensor(bow_en, dtype=torch.float32)\n",
    "    labels_en_tensor = torch.tensor(labels_en, dtype=torch.float32)\n",
    "    labels_c2e = torch.tensor(labels_c2e, dtype=torch.float32)\n",
    "    dataset_en = TensorDataset(embeddings_en, bow_en_tensor, labels_en_tensor, labels_c2e)\n",
    "    dataloader_en = DataLoader(dataset_en, batch_size=batch_size_en, shuffle=True)\n",
    "\n",
    "    # Convert Chinese data to tensors\n",
    "    embeddings_cn = torch.tensor(sbert_doc_embeddings_cn, dtype=torch.float32)\n",
    "    bow_cn_tensor = torch.tensor(bow_cn, dtype=torch.float32)\n",
    "    labels_cn_tensor = torch.tensor(labels_cn, dtype=torch.float32)\n",
    "    labels_e2c = torch.tensor(labels_e2c, dtype=torch.float32)\n",
    "    dataset_cn = TensorDataset(embeddings_cn, bow_cn_tensor, labels_cn_tensor, labels_e2c)\n",
    "    dataloader_cn = DataLoader(dataset_cn, batch_size=batch_size_cn, shuffle=True)\n",
    "\n",
    "    return dataloader_en, dataloader_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65c1efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RPSXTM(model, dataloader_en, dataloader_cn, optimizer, num_epochs=500, device='cpu'):\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Zip the two dataloaders to iterate over them simultaneously\n",
    "        for (x_en, x_en_bow, labels_en, labels_c2e), (x_cn, x_cn_bow, labels_cn, labels_e2c) in zip(dataloader_en, dataloader_cn):\n",
    "            # Move data to the specified device\n",
    "            x_en, x_en_bow, labels_en, labels_c2e = x_en.to(device), x_en_bow.to(device), labels_en.to(device), labels_c2e.to(device)\n",
    "            x_cn, x_cn_bow, labels_cn, labels_e2c = x_cn.to(device), x_cn_bow.to(device), labels_cn.to(device), labels_e2c.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_en, x_cn, x_en_bow, x_cn_bow, labels_en, labels_cn, labels_c2e, labels_e2c)\n",
    "\n",
    "            # Handle potential keys in the outputs\n",
    "            tm_loss = outputs.get('topic_modeling_loss', torch.tensor(0.0, device=device))\n",
    "            dcl_loss = outputs.get('contrastive_loss', torch.tensor(0.0, device=device))\n",
    "            total_loss = outputs.get('total_loss', tm_loss + dcl_loss)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += total_loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader_en)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "242cedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def train_RPSXTM(model, dataloader_en, dataloader_cn, optimizer, num_epochs=500, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()  # 에폭 시작 시간 기록\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Zip the two dataloaders to iterate over them simultaneously\n",
    "        for (x_en, x_en_bow, labels_en, labels_c2e), (x_cn, x_cn_bow, labels_cn, labels_e2c) in zip(dataloader_en, dataloader_cn):\n",
    "            # Move data to the specified device\n",
    "            x_en = x_en.to(device)\n",
    "            x_en_bow = x_en_bow.to(device)\n",
    "            labels_en = labels_en.to(device)\n",
    "            labels_c2e = labels_c2e.to(device)\n",
    "            x_cn = x_cn.to(device)\n",
    "            x_cn_bow = x_cn_bow.to(device)\n",
    "            labels_cn = labels_cn.to(device)\n",
    "            labels_e2c = labels_e2c.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_en, x_cn, x_en_bow, x_cn_bow, labels_en, labels_cn, labels_c2e, labels_e2c)\n",
    "\n",
    "            # Handle potential keys in the outputs\n",
    "            tm_loss = outputs.get('topic_modeling_loss', torch.tensor(0.0, device=device))\n",
    "            dcl_loss = outputs.get('contrastive_loss', torch.tensor(0.0, device=device))\n",
    "            total_loss = outputs.get('total_loss', tm_loss + dcl_loss)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += total_loss.item()\n",
    "\n",
    "        end_time = time.time()  # 에폭 종료 시간 기록\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "              f\"Loss: {epoch_loss / len(dataloader_en):.4f}, \"\n",
    "              f\"Time: {epoch_time:.2f} sec\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e54199bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 698.5715, Time: 1.72 sec\n",
      "Epoch 2/500, Loss: 702.4260, Time: 1.59 sec\n",
      "Epoch 3/500, Loss: 699.6259, Time: 1.57 sec\n",
      "Epoch 4/500, Loss: 695.0632, Time: 1.50 sec\n",
      "Epoch 5/500, Loss: 696.4761, Time: 1.50 sec\n",
      "Epoch 6/500, Loss: 692.0394, Time: 1.52 sec\n",
      "Epoch 7/500, Loss: 688.4872, Time: 1.66 sec\n",
      "Epoch 8/500, Loss: 686.8682, Time: 1.57 sec\n",
      "Epoch 9/500, Loss: 681.0563, Time: 1.54 sec\n",
      "Epoch 10/500, Loss: 674.9122, Time: 1.58 sec\n",
      "Epoch 11/500, Loss: 673.5864, Time: 1.60 sec\n",
      "Epoch 12/500, Loss: 669.7195, Time: 1.73 sec\n",
      "Epoch 13/500, Loss: 666.6424, Time: 1.57 sec\n",
      "Epoch 14/500, Loss: 664.6134, Time: 1.71 sec\n",
      "Epoch 15/500, Loss: 661.0383, Time: 1.53 sec\n",
      "Epoch 16/500, Loss: 655.8762, Time: 1.57 sec\n",
      "Epoch 17/500, Loss: 653.8022, Time: 1.60 sec\n",
      "Epoch 18/500, Loss: 652.6873, Time: 1.56 sec\n",
      "Epoch 19/500, Loss: 651.5659, Time: 1.64 sec\n",
      "Epoch 20/500, Loss: 648.3586, Time: 1.59 sec\n",
      "Epoch 21/500, Loss: 643.9933, Time: 1.57 sec\n",
      "Epoch 22/500, Loss: 645.2629, Time: 1.57 sec\n",
      "Epoch 23/500, Loss: 642.0941, Time: 1.55 sec\n",
      "Epoch 24/500, Loss: 639.7893, Time: 1.55 sec\n",
      "Epoch 25/500, Loss: 637.8400, Time: 1.54 sec\n",
      "Epoch 26/500, Loss: 639.4565, Time: 1.56 sec\n",
      "Epoch 27/500, Loss: 634.9630, Time: 1.52 sec\n",
      "Epoch 28/500, Loss: 633.2478, Time: 1.55 sec\n",
      "Epoch 29/500, Loss: 634.3807, Time: 2.42 sec\n",
      "Epoch 30/500, Loss: 633.6837, Time: 1.88 sec\n",
      "Epoch 31/500, Loss: 630.6577, Time: 1.63 sec\n",
      "Epoch 32/500, Loss: 630.9143, Time: 1.57 sec\n",
      "Epoch 33/500, Loss: 629.4733, Time: 1.52 sec\n",
      "Epoch 34/500, Loss: 626.6449, Time: 1.59 sec\n",
      "Epoch 35/500, Loss: 626.1850, Time: 1.55 sec\n",
      "Epoch 36/500, Loss: 627.3141, Time: 1.62 sec\n",
      "Epoch 37/500, Loss: 625.4995, Time: 1.59 sec\n",
      "Epoch 38/500, Loss: 622.9565, Time: 1.65 sec\n",
      "Epoch 39/500, Loss: 625.1416, Time: 1.71 sec\n",
      "Epoch 40/500, Loss: 623.2351, Time: 1.57 sec\n",
      "Epoch 41/500, Loss: 623.5033, Time: 1.56 sec\n",
      "Epoch 42/500, Loss: 622.1490, Time: 1.52 sec\n",
      "Epoch 43/500, Loss: 621.5901, Time: 1.56 sec\n",
      "Epoch 44/500, Loss: 623.4393, Time: 1.58 sec\n",
      "Epoch 45/500, Loss: 619.1105, Time: 1.67 sec\n",
      "Epoch 46/500, Loss: 622.0933, Time: 1.64 sec\n",
      "Epoch 47/500, Loss: 617.8955, Time: 1.70 sec\n",
      "Epoch 48/500, Loss: 619.3400, Time: 1.56 sec\n",
      "Epoch 49/500, Loss: 617.0963, Time: 1.69 sec\n",
      "Epoch 50/500, Loss: 616.4170, Time: 1.61 sec\n",
      "Epoch 51/500, Loss: 615.6200, Time: 1.58 sec\n",
      "Epoch 52/500, Loss: 614.7129, Time: 1.52 sec\n",
      "Epoch 53/500, Loss: 614.6646, Time: 1.51 sec\n",
      "Epoch 54/500, Loss: 612.7163, Time: 1.55 sec\n",
      "Epoch 55/500, Loss: 619.0276, Time: 1.51 sec\n",
      "Epoch 56/500, Loss: 611.5022, Time: 1.50 sec\n",
      "Epoch 57/500, Loss: 615.4509, Time: 1.58 sec\n",
      "Epoch 58/500, Loss: 612.8631, Time: 1.73 sec\n",
      "Epoch 59/500, Loss: 613.1758, Time: 1.57 sec\n",
      "Epoch 60/500, Loss: 612.0667, Time: 1.57 sec\n",
      "Epoch 61/500, Loss: 612.3278, Time: 1.53 sec\n",
      "Epoch 62/500, Loss: 611.7868, Time: 1.51 sec\n",
      "Epoch 63/500, Loss: 610.3007, Time: 1.51 sec\n",
      "Epoch 64/500, Loss: 609.6977, Time: 1.57 sec\n",
      "Epoch 65/500, Loss: 612.5592, Time: 1.61 sec\n",
      "Epoch 66/500, Loss: 611.4845, Time: 1.61 sec\n",
      "Epoch 67/500, Loss: 608.1578, Time: 1.54 sec\n",
      "Epoch 68/500, Loss: 610.5254, Time: 1.56 sec\n",
      "Epoch 69/500, Loss: 610.4807, Time: 1.56 sec\n",
      "Epoch 70/500, Loss: 607.5693, Time: 1.55 sec\n",
      "Epoch 71/500, Loss: 610.6466, Time: 1.54 sec\n",
      "Epoch 72/500, Loss: 607.4603, Time: 1.54 sec\n",
      "Epoch 73/500, Loss: 606.8197, Time: 1.58 sec\n",
      "Epoch 74/500, Loss: 607.0848, Time: 1.52 sec\n",
      "Epoch 75/500, Loss: 606.5611, Time: 1.52 sec\n",
      "Epoch 76/500, Loss: 607.5192, Time: 1.58 sec\n",
      "Epoch 77/500, Loss: 606.0764, Time: 1.57 sec\n",
      "Epoch 78/500, Loss: 605.7376, Time: 1.58 sec\n",
      "Epoch 79/500, Loss: 604.9287, Time: 1.52 sec\n",
      "Epoch 80/500, Loss: 606.9714, Time: 1.50 sec\n",
      "Epoch 81/500, Loss: 603.5272, Time: 1.50 sec\n",
      "Epoch 82/500, Loss: 604.5749, Time: 1.51 sec\n",
      "Epoch 83/500, Loss: 604.6574, Time: 1.55 sec\n",
      "Epoch 84/500, Loss: 606.2728, Time: 1.51 sec\n",
      "Epoch 85/500, Loss: 604.2761, Time: 1.56 sec\n",
      "Epoch 86/500, Loss: 605.1905, Time: 1.55 sec\n",
      "Epoch 87/500, Loss: 603.9785, Time: 1.55 sec\n",
      "Epoch 88/500, Loss: 605.1559, Time: 1.55 sec\n",
      "Epoch 89/500, Loss: 604.3093, Time: 1.51 sec\n",
      "Epoch 90/500, Loss: 601.4507, Time: 1.50 sec\n",
      "Epoch 91/500, Loss: 602.4663, Time: 1.51 sec\n",
      "Epoch 92/500, Loss: 601.8602, Time: 1.56 sec\n",
      "Epoch 93/500, Loss: 601.5273, Time: 1.50 sec\n",
      "Epoch 94/500, Loss: 600.9753, Time: 1.56 sec\n",
      "Epoch 95/500, Loss: 599.7513, Time: 1.50 sec\n",
      "Epoch 96/500, Loss: 600.1053, Time: 1.57 sec\n",
      "Epoch 97/500, Loss: 601.5534, Time: 1.56 sec\n",
      "Epoch 98/500, Loss: 598.1263, Time: 1.50 sec\n",
      "Epoch 99/500, Loss: 599.2354, Time: 1.51 sec\n",
      "Epoch 100/500, Loss: 598.3539, Time: 1.50 sec\n",
      "Epoch 101/500, Loss: 601.2070, Time: 1.59 sec\n",
      "Epoch 102/500, Loss: 597.9263, Time: 1.51 sec\n",
      "Epoch 103/500, Loss: 599.6494, Time: 1.51 sec\n",
      "Epoch 104/500, Loss: 597.6878, Time: 1.56 sec\n",
      "Epoch 105/500, Loss: 597.0182, Time: 1.56 sec\n",
      "Epoch 106/500, Loss: 597.2643, Time: 1.56 sec\n",
      "Epoch 107/500, Loss: 597.5065, Time: 1.52 sec\n",
      "Epoch 108/500, Loss: 595.7559, Time: 1.51 sec\n",
      "Epoch 109/500, Loss: 601.1126, Time: 1.52 sec\n",
      "Epoch 110/500, Loss: 598.3030, Time: 1.51 sec\n",
      "Epoch 111/500, Loss: 596.2242, Time: 1.58 sec\n",
      "Epoch 112/500, Loss: 598.8482, Time: 1.51 sec\n",
      "Epoch 113/500, Loss: 595.4685, Time: 1.56 sec\n",
      "Epoch 114/500, Loss: 595.9203, Time: 1.52 sec\n",
      "Epoch 115/500, Loss: 594.4831, Time: 1.58 sec\n",
      "Epoch 116/500, Loss: 596.0923, Time: 1.56 sec\n",
      "Epoch 117/500, Loss: 593.5585, Time: 1.51 sec\n",
      "Epoch 118/500, Loss: 592.8844, Time: 1.50 sec\n",
      "Epoch 119/500, Loss: 590.3497, Time: 1.51 sec\n",
      "Epoch 120/500, Loss: 588.1772, Time: 1.56 sec\n",
      "Epoch 121/500, Loss: 588.1603, Time: 1.51 sec\n",
      "Epoch 122/500, Loss: 589.7878, Time: 1.51 sec\n",
      "Epoch 123/500, Loss: 588.9549, Time: 1.56 sec\n",
      "Epoch 124/500, Loss: 586.4427, Time: 1.56 sec\n",
      "Epoch 125/500, Loss: 584.0552, Time: 1.56 sec\n",
      "Epoch 126/500, Loss: 582.9141, Time: 1.51 sec\n",
      "Epoch 127/500, Loss: 585.8444, Time: 1.51 sec\n",
      "Epoch 128/500, Loss: 583.7743, Time: 1.51 sec\n",
      "Epoch 129/500, Loss: 582.5553, Time: 1.51 sec\n",
      "Epoch 130/500, Loss: 583.3526, Time: 1.56 sec\n",
      "Epoch 131/500, Loss: 583.8865, Time: 1.51 sec\n",
      "Epoch 132/500, Loss: 584.4110, Time: 1.56 sec\n",
      "Epoch 133/500, Loss: 579.9095, Time: 1.56 sec\n",
      "Epoch 134/500, Loss: 583.2628, Time: 1.51 sec\n",
      "Epoch 135/500, Loss: 582.3366, Time: 1.56 sec\n",
      "Epoch 136/500, Loss: 584.2448, Time: 1.51 sec\n",
      "Epoch 137/500, Loss: 580.6885, Time: 1.51 sec\n",
      "Epoch 138/500, Loss: 579.5147, Time: 1.52 sec\n",
      "Epoch 139/500, Loss: 580.4902, Time: 1.56 sec\n",
      "Epoch 140/500, Loss: 583.5835, Time: 1.52 sec\n",
      "Epoch 141/500, Loss: 580.9214, Time: 1.57 sec\n",
      "Epoch 142/500, Loss: 580.1746, Time: 1.52 sec\n",
      "Epoch 143/500, Loss: 580.7603, Time: 1.57 sec\n",
      "Epoch 144/500, Loss: 581.9635, Time: 1.58 sec\n",
      "Epoch 145/500, Loss: 578.4131, Time: 1.53 sec\n",
      "Epoch 146/500, Loss: 579.6088, Time: 1.53 sec\n",
      "Epoch 147/500, Loss: 578.2309, Time: 1.53 sec\n",
      "Epoch 148/500, Loss: 581.0030, Time: 1.59 sec\n",
      "Epoch 149/500, Loss: 580.2401, Time: 1.53 sec\n",
      "Epoch 150/500, Loss: 579.1067, Time: 1.57 sec\n",
      "Epoch 151/500, Loss: 577.9721, Time: 1.59 sec\n",
      "Epoch 152/500, Loss: 576.6356, Time: 1.61 sec\n",
      "Epoch 153/500, Loss: 577.3079, Time: 1.61 sec\n",
      "Epoch 154/500, Loss: 580.3791, Time: 1.55 sec\n",
      "Epoch 155/500, Loss: 579.1771, Time: 1.57 sec\n",
      "Epoch 156/500, Loss: 579.5874, Time: 1.56 sec\n",
      "Epoch 157/500, Loss: 577.9030, Time: 1.57 sec\n",
      "Epoch 158/500, Loss: 575.5174, Time: 1.61 sec\n",
      "Epoch 159/500, Loss: 577.5276, Time: 1.56 sec\n",
      "Epoch 160/500, Loss: 576.4468, Time: 1.61 sec\n",
      "Epoch 161/500, Loss: 579.8320, Time: 1.57 sec\n",
      "Epoch 162/500, Loss: 577.5513, Time: 1.63 sec\n",
      "Epoch 163/500, Loss: 576.8524, Time: 1.63 sec\n",
      "Epoch 164/500, Loss: 575.7107, Time: 1.58 sec\n",
      "Epoch 165/500, Loss: 575.6442, Time: 1.59 sec\n",
      "Epoch 166/500, Loss: 577.7240, Time: 1.59 sec\n",
      "Epoch 167/500, Loss: 579.1024, Time: 1.64 sec\n",
      "Epoch 168/500, Loss: 575.1442, Time: 1.69 sec\n",
      "Epoch 169/500, Loss: 577.1484, Time: 1.71 sec\n",
      "Epoch 170/500, Loss: 575.8340, Time: 1.64 sec\n",
      "Epoch 171/500, Loss: 574.5746, Time: 1.68 sec\n",
      "Epoch 172/500, Loss: 576.4620, Time: 1.65 sec\n",
      "Epoch 173/500, Loss: 574.7072, Time: 1.66 sec\n",
      "Epoch 174/500, Loss: 577.4694, Time: 1.63 sec\n",
      "Epoch 175/500, Loss: 574.3669, Time: 1.61 sec\n",
      "Epoch 176/500, Loss: 575.2721, Time: 1.61 sec\n",
      "Epoch 177/500, Loss: 576.0360, Time: 1.66 sec\n",
      "Epoch 178/500, Loss: 575.5502, Time: 1.61 sec\n",
      "Epoch 179/500, Loss: 572.6997, Time: 1.65 sec\n",
      "Epoch 180/500, Loss: 574.4692, Time: 1.68 sec\n",
      "Epoch 181/500, Loss: 575.0435, Time: 1.61 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/500, Loss: 574.1979, Time: 1.66 sec\n",
      "Epoch 183/500, Loss: 575.2690, Time: 1.60 sec\n",
      "Epoch 184/500, Loss: 573.1657, Time: 1.63 sec\n",
      "Epoch 185/500, Loss: 574.2065, Time: 1.67 sec\n",
      "Epoch 186/500, Loss: 574.3806, Time: 1.69 sec\n",
      "Epoch 187/500, Loss: 574.8213, Time: 1.62 sec\n",
      "Epoch 188/500, Loss: 574.8298, Time: 1.67 sec\n",
      "Epoch 189/500, Loss: 575.5063, Time: 1.62 sec\n",
      "Epoch 190/500, Loss: 574.4574, Time: 1.68 sec\n",
      "Epoch 191/500, Loss: 572.1597, Time: 1.66 sec\n",
      "Epoch 192/500, Loss: 574.4739, Time: 1.62 sec\n",
      "Epoch 193/500, Loss: 572.7712, Time: 1.61 sec\n",
      "Epoch 194/500, Loss: 572.0812, Time: 1.61 sec\n",
      "Epoch 195/500, Loss: 573.4064, Time: 1.67 sec\n",
      "Epoch 196/500, Loss: 573.1377, Time: 1.61 sec\n",
      "Epoch 197/500, Loss: 572.2444, Time: 1.61 sec\n",
      "Epoch 198/500, Loss: 571.2118, Time: 1.66 sec\n",
      "Epoch 199/500, Loss: 572.0771, Time: 1.70 sec\n",
      "Epoch 200/500, Loss: 569.8692, Time: 1.66 sec\n",
      "Epoch 201/500, Loss: 574.9997, Time: 1.60 sec\n",
      "Epoch 202/500, Loss: 572.7728, Time: 1.61 sec\n",
      "Epoch 203/500, Loss: 570.8668, Time: 1.62 sec\n",
      "Epoch 204/500, Loss: 570.2842, Time: 1.61 sec\n",
      "Epoch 205/500, Loss: 570.8888, Time: 1.66 sec\n",
      "Epoch 206/500, Loss: 570.9529, Time: 1.61 sec\n",
      "Epoch 207/500, Loss: 570.3820, Time: 1.65 sec\n",
      "Epoch 208/500, Loss: 571.1492, Time: 1.61 sec\n",
      "Epoch 209/500, Loss: 571.5482, Time: 1.65 sec\n",
      "Epoch 210/500, Loss: 568.9675, Time: 1.67 sec\n",
      "Epoch 211/500, Loss: 569.9957, Time: 1.61 sec\n",
      "Epoch 212/500, Loss: 570.3542, Time: 1.63 sec\n",
      "Epoch 213/500, Loss: 569.0956, Time: 1.61 sec\n",
      "Epoch 214/500, Loss: 569.7223, Time: 1.67 sec\n",
      "Epoch 215/500, Loss: 571.0292, Time: 1.61 sec\n",
      "Epoch 216/500, Loss: 569.3900, Time: 1.66 sec\n",
      "Epoch 217/500, Loss: 572.1197, Time: 1.60 sec\n",
      "Epoch 218/500, Loss: 569.4323, Time: 1.65 sec\n",
      "Epoch 219/500, Loss: 569.3785, Time: 1.65 sec\n",
      "Epoch 220/500, Loss: 567.4540, Time: 1.63 sec\n",
      "Epoch 221/500, Loss: 569.3071, Time: 1.61 sec\n",
      "Epoch 222/500, Loss: 570.4277, Time: 1.61 sec\n",
      "Epoch 223/500, Loss: 574.1199, Time: 1.61 sec\n",
      "Epoch 224/500, Loss: 570.7444, Time: 1.65 sec\n",
      "Epoch 225/500, Loss: 567.9588, Time: 1.61 sec\n",
      "Epoch 226/500, Loss: 570.2614, Time: 1.66 sec\n",
      "Epoch 227/500, Loss: 566.3810, Time: 1.67 sec\n",
      "Epoch 228/500, Loss: 569.7835, Time: 1.60 sec\n",
      "Epoch 229/500, Loss: 566.4815, Time: 1.66 sec\n",
      "Epoch 230/500, Loss: 567.5671, Time: 1.60 sec\n",
      "Epoch 231/500, Loss: 567.8248, Time: 1.62 sec\n",
      "Epoch 232/500, Loss: 564.4810, Time: 1.60 sec\n",
      "Epoch 233/500, Loss: 569.8733, Time: 1.65 sec\n",
      "Epoch 234/500, Loss: 571.1290, Time: 1.60 sec\n",
      "Epoch 235/500, Loss: 568.5735, Time: 1.65 sec\n",
      "Epoch 236/500, Loss: 569.6769, Time: 1.60 sec\n",
      "Epoch 237/500, Loss: 565.8843, Time: 1.65 sec\n",
      "Epoch 238/500, Loss: 567.5208, Time: 1.65 sec\n",
      "Epoch 239/500, Loss: 568.2231, Time: 1.60 sec\n",
      "Epoch 240/500, Loss: 567.5011, Time: 1.60 sec\n",
      "Epoch 241/500, Loss: 567.0848, Time: 1.60 sec\n",
      "Epoch 242/500, Loss: 565.8687, Time: 1.65 sec\n",
      "Epoch 243/500, Loss: 568.1553, Time: 1.59 sec\n",
      "Epoch 244/500, Loss: 565.3829, Time: 1.60 sec\n",
      "Epoch 245/500, Loss: 570.2070, Time: 1.64 sec\n",
      "Epoch 246/500, Loss: 569.0419, Time: 1.65 sec\n",
      "Epoch 247/500, Loss: 566.4462, Time: 1.70 sec\n",
      "Epoch 248/500, Loss: 564.0139, Time: 1.60 sec\n",
      "Epoch 249/500, Loss: 565.9463, Time: 1.60 sec\n",
      "Epoch 250/500, Loss: 565.3395, Time: 1.60 sec\n",
      "Epoch 251/500, Loss: 564.2326, Time: 1.60 sec\n",
      "Epoch 252/500, Loss: 567.4653, Time: 1.64 sec\n",
      "Epoch 253/500, Loss: 566.1276, Time: 1.60 sec\n",
      "Epoch 254/500, Loss: 567.4244, Time: 1.64 sec\n",
      "Epoch 255/500, Loss: 566.2965, Time: 1.60 sec\n",
      "Epoch 256/500, Loss: 563.4424, Time: 1.64 sec\n",
      "Epoch 257/500, Loss: 564.8132, Time: 1.64 sec\n",
      "Epoch 258/500, Loss: 565.8326, Time: 1.60 sec\n",
      "Epoch 259/500, Loss: 567.3210, Time: 1.60 sec\n",
      "Epoch 260/500, Loss: 566.9782, Time: 1.61 sec\n",
      "Epoch 261/500, Loss: 564.0362, Time: 1.64 sec\n",
      "Epoch 262/500, Loss: 565.3887, Time: 1.60 sec\n",
      "Epoch 263/500, Loss: 566.5909, Time: 1.65 sec\n",
      "Epoch 264/500, Loss: 566.2186, Time: 1.60 sec\n",
      "Epoch 265/500, Loss: 562.5964, Time: 1.64 sec\n",
      "Epoch 266/500, Loss: 565.8883, Time: 1.64 sec\n",
      "Epoch 267/500, Loss: 565.2844, Time: 1.60 sec\n",
      "Epoch 268/500, Loss: 562.6856, Time: 1.61 sec\n",
      "Epoch 269/500, Loss: 563.2382, Time: 1.73 sec\n",
      "Epoch 270/500, Loss: 565.5265, Time: 1.62 sec\n",
      "Epoch 271/500, Loss: 562.8655, Time: 1.64 sec\n",
      "Epoch 272/500, Loss: 566.9050, Time: 1.61 sec\n",
      "Epoch 273/500, Loss: 563.6086, Time: 1.65 sec\n",
      "Epoch 274/500, Loss: 562.5702, Time: 1.65 sec\n",
      "Epoch 275/500, Loss: 561.1403, Time: 1.59 sec\n",
      "Epoch 276/500, Loss: 561.2126, Time: 1.65 sec\n",
      "Epoch 277/500, Loss: 562.4797, Time: 1.60 sec\n",
      "Epoch 278/500, Loss: 562.6072, Time: 1.60 sec\n",
      "Epoch 279/500, Loss: 564.2153, Time: 1.61 sec\n",
      "Epoch 280/500, Loss: 563.9992, Time: 1.64 sec\n",
      "Epoch 281/500, Loss: 562.4612, Time: 1.60 sec\n",
      "Epoch 282/500, Loss: 564.8770, Time: 1.65 sec\n",
      "Epoch 283/500, Loss: 562.4729, Time: 1.60 sec\n",
      "Epoch 284/500, Loss: 562.8135, Time: 1.66 sec\n",
      "Epoch 285/500, Loss: 563.6085, Time: 1.64 sec\n",
      "Epoch 286/500, Loss: 560.2798, Time: 1.60 sec\n",
      "Epoch 287/500, Loss: 562.3070, Time: 1.60 sec\n",
      "Epoch 288/500, Loss: 561.2117, Time: 1.60 sec\n",
      "Epoch 289/500, Loss: 561.8864, Time: 1.65 sec\n",
      "Epoch 290/500, Loss: 561.9729, Time: 1.60 sec\n",
      "Epoch 291/500, Loss: 563.7084, Time: 1.60 sec\n",
      "Epoch 292/500, Loss: 562.9724, Time: 1.65 sec\n",
      "Epoch 293/500, Loss: 562.4029, Time: 1.64 sec\n",
      "Epoch 294/500, Loss: 562.9085, Time: 1.65 sec\n",
      "Epoch 295/500, Loss: 560.4865, Time: 1.59 sec\n",
      "Epoch 296/500, Loss: 561.5254, Time: 1.60 sec\n",
      "Epoch 297/500, Loss: 561.2576, Time: 1.60 sec\n",
      "Epoch 298/500, Loss: 560.7500, Time: 1.60 sec\n",
      "Epoch 299/500, Loss: 561.5261, Time: 1.65 sec\n",
      "Epoch 300/500, Loss: 560.3766, Time: 1.60 sec\n",
      "Epoch 301/500, Loss: 561.7322, Time: 1.65 sec\n",
      "Epoch 302/500, Loss: 560.4879, Time: 1.60 sec\n",
      "Epoch 303/500, Loss: 563.2451, Time: 1.69 sec\n",
      "Epoch 304/500, Loss: 561.1859, Time: 1.71 sec\n",
      "Epoch 305/500, Loss: 560.1076, Time: 1.60 sec\n",
      "Epoch 306/500, Loss: 559.3925, Time: 1.62 sec\n",
      "Epoch 307/500, Loss: 562.6905, Time: 1.60 sec\n",
      "Epoch 308/500, Loss: 559.6296, Time: 1.64 sec\n",
      "Epoch 309/500, Loss: 561.3291, Time: 1.60 sec\n",
      "Epoch 310/500, Loss: 563.8698, Time: 1.66 sec\n",
      "Epoch 311/500, Loss: 560.6173, Time: 1.60 sec\n",
      "Epoch 312/500, Loss: 559.0151, Time: 1.65 sec\n",
      "Epoch 313/500, Loss: 561.6439, Time: 1.64 sec\n",
      "Epoch 314/500, Loss: 560.1256, Time: 1.62 sec\n",
      "Epoch 315/500, Loss: 559.5010, Time: 1.60 sec\n",
      "Epoch 316/500, Loss: 557.3596, Time: 1.63 sec\n",
      "Epoch 317/500, Loss: 560.1845, Time: 1.61 sec\n",
      "Epoch 318/500, Loss: 560.8331, Time: 1.66 sec\n",
      "Epoch 319/500, Loss: 559.9293, Time: 1.60 sec\n",
      "Epoch 320/500, Loss: 559.2954, Time: 1.66 sec\n",
      "Epoch 321/500, Loss: 560.4663, Time: 1.66 sec\n",
      "Epoch 322/500, Loss: 558.7610, Time: 1.61 sec\n",
      "Epoch 323/500, Loss: 559.5530, Time: 1.64 sec\n",
      "Epoch 324/500, Loss: 560.2260, Time: 1.60 sec\n",
      "Epoch 325/500, Loss: 557.9075, Time: 1.64 sec\n",
      "Epoch 326/500, Loss: 560.7568, Time: 1.60 sec\n",
      "Epoch 327/500, Loss: 560.1380, Time: 1.65 sec\n",
      "Epoch 328/500, Loss: 560.6167, Time: 1.60 sec\n",
      "Epoch 329/500, Loss: 559.5119, Time: 1.65 sec\n",
      "Epoch 330/500, Loss: 557.1162, Time: 1.62 sec\n",
      "Epoch 331/500, Loss: 560.9312, Time: 1.70 sec\n",
      "Epoch 332/500, Loss: 558.6021, Time: 1.69 sec\n",
      "Epoch 333/500, Loss: 559.9899, Time: 1.71 sec\n",
      "Epoch 334/500, Loss: 560.8409, Time: 1.62 sec\n",
      "Epoch 335/500, Loss: 559.3129, Time: 1.61 sec\n",
      "Epoch 336/500, Loss: 559.6423, Time: 1.65 sec\n",
      "Epoch 337/500, Loss: 559.3768, Time: 1.60 sec\n",
      "Epoch 338/500, Loss: 559.2705, Time: 1.60 sec\n",
      "Epoch 339/500, Loss: 558.5194, Time: 1.65 sec\n",
      "Epoch 340/500, Loss: 559.0958, Time: 1.65 sec\n",
      "Epoch 341/500, Loss: 559.3073, Time: 1.65 sec\n",
      "Epoch 342/500, Loss: 556.7169, Time: 1.60 sec\n",
      "Epoch 343/500, Loss: 558.8312, Time: 1.61 sec\n",
      "Epoch 344/500, Loss: 557.5901, Time: 1.66 sec\n",
      "Epoch 345/500, Loss: 557.3732, Time: 1.65 sec\n",
      "Epoch 346/500, Loss: 559.2842, Time: 1.68 sec\n",
      "Epoch 347/500, Loss: 559.3319, Time: 1.71 sec\n",
      "Epoch 348/500, Loss: 558.3462, Time: 1.68 sec\n",
      "Epoch 349/500, Loss: 557.5979, Time: 1.62 sec\n",
      "Epoch 350/500, Loss: 557.4062, Time: 1.85 sec\n",
      "Epoch 351/500, Loss: 559.0387, Time: 1.80 sec\n",
      "Epoch 352/500, Loss: 558.5256, Time: 1.62 sec\n",
      "Epoch 353/500, Loss: 555.7291, Time: 1.68 sec\n",
      "Epoch 354/500, Loss: 559.1384, Time: 1.67 sec\n",
      "Epoch 355/500, Loss: 555.7533, Time: 1.86 sec\n",
      "Epoch 356/500, Loss: 557.1412, Time: 1.74 sec\n",
      "Epoch 357/500, Loss: 560.3620, Time: 1.73 sec\n",
      "Epoch 358/500, Loss: 556.4131, Time: 1.63 sec\n",
      "Epoch 359/500, Loss: 555.0458, Time: 1.67 sec\n",
      "Epoch 360/500, Loss: 556.8404, Time: 1.66 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/500, Loss: 558.7311, Time: 1.61 sec\n",
      "Epoch 362/500, Loss: 557.2483, Time: 1.61 sec\n",
      "Epoch 363/500, Loss: 556.3551, Time: 1.62 sec\n",
      "Epoch 364/500, Loss: 556.5291, Time: 1.61 sec\n",
      "Epoch 365/500, Loss: 557.9483, Time: 1.67 sec\n",
      "Epoch 366/500, Loss: 557.2259, Time: 1.61 sec\n",
      "Epoch 367/500, Loss: 556.6408, Time: 1.66 sec\n",
      "Epoch 368/500, Loss: 558.5637, Time: 1.66 sec\n",
      "Epoch 369/500, Loss: 555.5112, Time: 1.65 sec\n",
      "Epoch 370/500, Loss: 554.4832, Time: 1.73 sec\n",
      "Epoch 371/500, Loss: 554.9550, Time: 1.62 sec\n",
      "Epoch 372/500, Loss: 555.8599, Time: 1.63 sec\n",
      "Epoch 373/500, Loss: 555.1079, Time: 1.62 sec\n",
      "Epoch 374/500, Loss: 555.5967, Time: 1.67 sec\n",
      "Epoch 375/500, Loss: 557.5265, Time: 1.61 sec\n",
      "Epoch 376/500, Loss: 557.9375, Time: 1.68 sec\n",
      "Epoch 377/500, Loss: 559.7440, Time: 1.61 sec\n",
      "Epoch 378/500, Loss: 555.5316, Time: 1.66 sec\n",
      "Epoch 379/500, Loss: 556.1356, Time: 1.66 sec\n",
      "Epoch 380/500, Loss: 555.4844, Time: 1.61 sec\n",
      "Epoch 381/500, Loss: 554.5845, Time: 1.61 sec\n",
      "Epoch 382/500, Loss: 556.4739, Time: 1.61 sec\n",
      "Epoch 383/500, Loss: 554.6671, Time: 1.66 sec\n",
      "Epoch 384/500, Loss: 553.7231, Time: 1.61 sec\n",
      "Epoch 385/500, Loss: 555.4971, Time: 1.61 sec\n",
      "Epoch 386/500, Loss: 555.8095, Time: 1.66 sec\n",
      "Epoch 387/500, Loss: 555.6309, Time: 1.65 sec\n",
      "Epoch 388/500, Loss: 558.2209, Time: 1.66 sec\n",
      "Epoch 389/500, Loss: 554.9424, Time: 1.61 sec\n",
      "Epoch 390/500, Loss: 558.3186, Time: 1.61 sec\n",
      "Epoch 391/500, Loss: 556.6295, Time: 1.61 sec\n",
      "Epoch 392/500, Loss: 554.6888, Time: 1.61 sec\n",
      "Epoch 393/500, Loss: 558.6861, Time: 1.70 sec\n",
      "Epoch 394/500, Loss: 555.0615, Time: 1.72 sec\n",
      "Epoch 395/500, Loss: 556.2339, Time: 1.83 sec\n",
      "Epoch 396/500, Loss: 554.3990, Time: 1.75 sec\n",
      "Epoch 397/500, Loss: 557.0412, Time: 1.84 sec\n",
      "Epoch 398/500, Loss: 554.9267, Time: 1.84 sec\n",
      "Epoch 399/500, Loss: 554.6386, Time: 1.82 sec\n",
      "Epoch 400/500, Loss: 554.2998, Time: 1.79 sec\n",
      "Epoch 401/500, Loss: 556.0306, Time: 1.88 sec\n",
      "Epoch 402/500, Loss: 557.9407, Time: 1.68 sec\n",
      "Epoch 403/500, Loss: 553.6375, Time: 1.62 sec\n",
      "Epoch 404/500, Loss: 553.9640, Time: 1.68 sec\n",
      "Epoch 405/500, Loss: 553.5049, Time: 1.62 sec\n",
      "Epoch 406/500, Loss: 552.9932, Time: 1.80 sec\n",
      "Epoch 407/500, Loss: 554.8558, Time: 2.52 sec\n",
      "Epoch 408/500, Loss: 554.0613, Time: 1.71 sec\n",
      "Epoch 409/500, Loss: 553.4616, Time: 1.73 sec\n",
      "Epoch 410/500, Loss: 555.8058, Time: 1.71 sec\n",
      "Epoch 411/500, Loss: 553.7149, Time: 1.67 sec\n",
      "Epoch 412/500, Loss: 552.6369, Time: 1.77 sec\n",
      "Epoch 413/500, Loss: 554.7570, Time: 1.84 sec\n",
      "Epoch 414/500, Loss: 553.7807, Time: 1.81 sec\n",
      "Epoch 415/500, Loss: 553.5760, Time: 1.73 sec\n",
      "Epoch 416/500, Loss: 552.9992, Time: 1.73 sec\n",
      "Epoch 417/500, Loss: 551.8435, Time: 1.78 sec\n",
      "Epoch 418/500, Loss: 555.2306, Time: 1.68 sec\n",
      "Epoch 419/500, Loss: 554.4257, Time: 1.66 sec\n",
      "Epoch 420/500, Loss: 553.4471, Time: 1.68 sec\n",
      "Epoch 421/500, Loss: 554.5142, Time: 1.79 sec\n",
      "Epoch 422/500, Loss: 554.2349, Time: 1.69 sec\n",
      "Epoch 423/500, Loss: 553.9362, Time: 1.91 sec\n",
      "Epoch 424/500, Loss: 554.0073, Time: 1.85 sec\n",
      "Epoch 425/500, Loss: 551.1106, Time: 1.77 sec\n",
      "Epoch 426/500, Loss: 550.4243, Time: 1.76 sec\n",
      "Epoch 427/500, Loss: 552.2451, Time: 1.73 sec\n",
      "Epoch 428/500, Loss: 554.6454, Time: 1.79 sec\n",
      "Epoch 429/500, Loss: 553.0288, Time: 1.72 sec\n",
      "Epoch 430/500, Loss: 553.7855, Time: 1.78 sec\n",
      "Epoch 431/500, Loss: 553.3466, Time: 1.74 sec\n",
      "Epoch 432/500, Loss: 550.9260, Time: 1.74 sec\n",
      "Epoch 433/500, Loss: 551.9506, Time: 1.76 sec\n",
      "Epoch 434/500, Loss: 552.3918, Time: 1.73 sec\n",
      "Epoch 435/500, Loss: 553.4273, Time: 1.74 sec\n",
      "Epoch 436/500, Loss: 553.0245, Time: 1.69 sec\n",
      "Epoch 437/500, Loss: 554.2153, Time: 1.68 sec\n",
      "Epoch 438/500, Loss: 550.8816, Time: 1.68 sec\n",
      "Epoch 439/500, Loss: 552.0595, Time: 1.68 sec\n",
      "Epoch 440/500, Loss: 551.8571, Time: 1.74 sec\n",
      "Epoch 441/500, Loss: 553.6830, Time: 1.66 sec\n",
      "Epoch 442/500, Loss: 551.4096, Time: 1.72 sec\n",
      "Epoch 443/500, Loss: 554.4666, Time: 1.67 sec\n",
      "Epoch 444/500, Loss: 553.6935, Time: 1.74 sec\n",
      "Epoch 445/500, Loss: 553.0984, Time: 1.83 sec\n",
      "Epoch 446/500, Loss: 555.2618, Time: 1.87 sec\n",
      "Epoch 447/500, Loss: 554.9813, Time: 1.69 sec\n",
      "Epoch 448/500, Loss: 554.0655, Time: 1.77 sec\n",
      "Epoch 449/500, Loss: 551.9519, Time: 1.84 sec\n",
      "Epoch 450/500, Loss: 550.1052, Time: 1.79 sec\n",
      "Epoch 451/500, Loss: 553.1603, Time: 1.75 sec\n",
      "Epoch 452/500, Loss: 550.0891, Time: 1.69 sec\n",
      "Epoch 453/500, Loss: 548.6662, Time: 1.75 sec\n",
      "Epoch 454/500, Loss: 550.0738, Time: 1.75 sec\n",
      "Epoch 455/500, Loss: 551.3329, Time: 1.69 sec\n",
      "Epoch 456/500, Loss: 551.3328, Time: 1.67 sec\n",
      "Epoch 457/500, Loss: 551.2342, Time: 1.67 sec\n",
      "Epoch 458/500, Loss: 552.1762, Time: 1.66 sec\n",
      "Epoch 459/500, Loss: 552.3633, Time: 1.70 sec\n",
      "Epoch 460/500, Loss: 550.4074, Time: 1.66 sec\n",
      "Epoch 461/500, Loss: 551.7459, Time: 1.86 sec\n",
      "Epoch 462/500, Loss: 551.3083, Time: 1.86 sec\n",
      "Epoch 463/500, Loss: 551.1060, Time: 1.67 sec\n",
      "Epoch 464/500, Loss: 549.6056, Time: 1.74 sec\n",
      "Epoch 465/500, Loss: 553.8565, Time: 1.69 sec\n",
      "Epoch 466/500, Loss: 551.0309, Time: 1.69 sec\n",
      "Epoch 467/500, Loss: 550.5525, Time: 1.67 sec\n",
      "Epoch 468/500, Loss: 550.5881, Time: 1.71 sec\n",
      "Epoch 469/500, Loss: 552.4239, Time: 1.66 sec\n",
      "Epoch 470/500, Loss: 552.5864, Time: 1.78 sec\n",
      "Epoch 471/500, Loss: 552.4276, Time: 1.67 sec\n",
      "Epoch 472/500, Loss: 549.8741, Time: 1.70 sec\n",
      "Epoch 473/500, Loss: 549.8837, Time: 1.71 sec\n",
      "Epoch 474/500, Loss: 549.4383, Time: 1.65 sec\n",
      "Epoch 475/500, Loss: 550.3329, Time: 1.66 sec\n",
      "Epoch 476/500, Loss: 553.7818, Time: 1.68 sec\n",
      "Epoch 477/500, Loss: 549.6325, Time: 1.71 sec\n",
      "Epoch 478/500, Loss: 550.1902, Time: 1.66 sec\n",
      "Epoch 479/500, Loss: 550.8270, Time: 1.66 sec\n",
      "Epoch 480/500, Loss: 551.6813, Time: 1.70 sec\n",
      "Epoch 481/500, Loss: 550.0299, Time: 1.70 sec\n",
      "Epoch 482/500, Loss: 551.2925, Time: 1.71 sec\n",
      "Epoch 483/500, Loss: 548.3939, Time: 1.66 sec\n",
      "Epoch 484/500, Loss: 550.5424, Time: 1.65 sec\n",
      "Epoch 485/500, Loss: 548.6827, Time: 1.65 sec\n",
      "Epoch 486/500, Loss: 549.0483, Time: 1.65 sec\n",
      "Epoch 487/500, Loss: 550.1268, Time: 1.70 sec\n",
      "Epoch 488/500, Loss: 552.5225, Time: 1.68 sec\n",
      "Epoch 489/500, Loss: 552.0245, Time: 2.33 sec\n",
      "Epoch 490/500, Loss: 549.3339, Time: 1.90 sec\n",
      "Epoch 491/500, Loss: 551.6230, Time: 1.79 sec\n",
      "Epoch 492/500, Loss: 547.8169, Time: 1.83 sec\n",
      "Epoch 493/500, Loss: 547.4143, Time: 1.81 sec\n",
      "Epoch 494/500, Loss: 551.6176, Time: 1.80 sec\n",
      "Epoch 495/500, Loss: 551.1773, Time: 1.75 sec\n",
      "Epoch 496/500, Loss: 548.8804, Time: 1.79 sec\n",
      "Epoch 497/500, Loss: 548.6196, Time: 2.10 sec\n",
      "Epoch 498/500, Loss: 548.7139, Time: 1.85 sec\n",
      "Epoch 499/500, Loss: 549.7211, Time: 1.74 sec\n",
      "Epoch 500/500, Loss: 549.2765, Time: 1.77 sec\n"
     ]
    }
   ],
   "source": [
    "# 사용 예제 on ECNews\n",
    "if __name__ == \"__main__\":\n",
    "    # SBERT 문서 임베딩 및 BoW 표현 로드\n",
    "    doc_embeddings_en_path = \"/users/seung-won/documents/datasets/Amazon_Review/AR_sbert_doc_embeddings_en.npz\"\n",
    "    doc_embeddings_cn_path = \"/users/seung-won/documents/datasets/Amazon_Review/AR_sbert_doc_embeddings_cn.npz\"\n",
    "    \n",
    "    # doc_embeddings_en_path = \"/users/seung-won/documents/datasets/Amazon_Review/AR_sbert_doc_embeddings_en.npz\"\n",
    "    # doc_embeddings_cn_path = \"/users/seung-won/documents/datasets/Amazon_Review/AR_sbert_doc_embeddings_cn.npz\"\n",
    "    \n",
    "    # bow_embeddings_en_path = \"/users/seung-won/documents/datasets/ECNews/ECN_bow_embeddings_en.npy\"\n",
    "    # bow_embeddings_cn_path = \"/users/seung-won/documents/datasets/ECNews/ECN_bow_embeddings_cn.npy\"\n",
    "    \n",
    "    bow_embeddings_en_path = \"/users/seung-won/documents/datasets/Amazon_Review/AR_bow_embeddings_en.npy\"\n",
    "    bow_embeddings_cn_path = \"/users/seung-won/documents/datasets/Amazon_Review/AR_bow_embeddings_cn.npy\"\n",
    "\n",
    "    labels_en_path = \"/users/seung-won/documents/TPL_method/data/Amazon_Review/XLM_labels_en_50.npy\"\n",
    "    labels_cn_path = \"/users/seung-won/documents/TPL_method/data/Amazon_Review/XLM_labels_cn_50.npy\"\n",
    "    \n",
    "    # labels_en_path = \"/users/seung-won/documents/TPL_method/data/Amazon_Review/k=50/labels_en.npy\"\n",
    "    # labels_cn_path = \"/users/seung-won/documents/TPL_method/data/Amazon_Review/k=50/labels_cn.npy\"\n",
    "\n",
    "    \n",
    "    # labels_c2e_path = \"/users/seung-won/documents/RPS/data/ECNews/labels_c2e_70.npy\"\n",
    "    # labels_e2c_path = \"/users/seung-won/documents/RPS/data/ECNews/labels_e2c_70.npy\"\n",
    "    \n",
    "    labels_c2e_path = \"/users/seung-won/documents/RPS/data/Amazon_Review/XLM_labels_c2e_30.npy\"\n",
    "    labels_e2c_path = \"/users/seung-won/documents/RPS/data/Amazon_Review/XLM_labels_e2c_30.npy\"\n",
    "\n",
    "    sbert_doc_embeddings_en = np.load(doc_embeddings_en_path)  # English SBERT embeddings\n",
    "    sbert_doc_embeddings_cn = np.load(doc_embeddings_cn_path)  # Chinese SBERT embeddings\n",
    "    \n",
    "    sbert_doc_embeddings_en = sbert_doc_embeddings_en['embeddings']\n",
    "    sbert_doc_embeddings_cn = sbert_doc_embeddings_cn['embeddings']\n",
    "    \n",
    "    bow_en = np.load(bow_embeddings_en_path)  # English BoW embeddings\n",
    "    bow_cn = np.load(bow_embeddings_cn_path)  # Chinese BoW embeddings\n",
    "    \n",
    "    labels_en = np.load(labels_en_path)\n",
    "    labels_cn = np.load(labels_cn_path)\n",
    "    \n",
    "    labels_c2e = np.load(labels_c2e_path)\n",
    "    labels_e2c = np.load(labels_e2c_path)\n",
    "    \n",
    "\n",
    "    # 모델 초기화\n",
    "    input_size = sbert_doc_embeddings_en.shape[1]\n",
    "    vocab_size_en = bow_en.shape[1]\n",
    "    vocab_size_cn = bow_cn.shape[1]\n",
    "    num_topics = 20\n",
    "    DCL_weight = 1\n",
    "    temperature = 0.1\n",
    "    \n",
    "    model = RPS_XTM(input_size=input_size, vocab_size_en=vocab_size_en, vocab_size_cn=vocab_size_cn,\n",
    "                    num_topics=num_topics, DCL_weight=DCL_weight,\n",
    "                    temperature=temperature, en_units=200, dropout=0.1)\n",
    "\n",
    "    # Optimizer 정의\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    batch_size_en = 512\n",
    "    batch_size_cn = 512\n",
    "    \n",
    "    dataloader_en, dataloader_cn = create_dataloader_separate(sbert_doc_embeddings_en, bow_en, labels_en,\n",
    "                                                              labels_c2e, sbert_doc_embeddings_cn, bow_cn,\n",
    "                                                              labels_cn, labels_e2c,\n",
    "                                                              batch_size_en, batch_size_cn)\n",
    "\n",
    "    # 모델 학습\n",
    "    trained_model = train_RPSXTM(model, dataloader_en, dataloader_cn, optimizer, num_epochs=500, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "535840bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words saved to 'top_words_en.txt' and 'top_words_cn.txt'\n"
     ]
    }
   ],
   "source": [
    "def save_top_words(beta, vocab, file_path, top_n=15):\n",
    "\n",
    "    # Convert beta to numpy array\n",
    "    beta_np = beta.detach().cpu().numpy()\n",
    "\n",
    "    # Open file for writing\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for topic_idx, topic_dist in enumerate(beta_np):\n",
    "            # Get top N word indices for the topic\n",
    "            top_word_indices = topic_dist.argsort()[-top_n:][::-1]\n",
    "            # Map indices to words\n",
    "            top_words = [vocab[idx] for idx in top_word_indices]\n",
    "            # Write topic and words to file\n",
    "            f.write(\" \".join(top_words) + \"\\n\")\n",
    "\n",
    "# 사용 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # 모델에서 beta_en, beta_cn 가져오기\n",
    "    beta_en, beta_cn = trained_model.get_beta()\n",
    "\n",
    "    # 영어와 중국어의 vocabulary 로드\n",
    "    vocab_en = [line.strip() for line in open(\"/users/seung-won/documents/datasets/Amazon_Review/AR_vocab_en\", encoding=\"utf-8\").readlines()]\n",
    "    vocab_cn = [line.strip() for line in open(\"/users/seung-won/documents/datasets/Amazon_Review/AR_vocab_cn\", encoding=\"utf-8\").readlines()]\n",
    "\n",
    "    # 상위 15개 단어를 각각 txt 파일로 저장\n",
    "    save_top_words(beta_en, vocab_en, \"/users/seung-won/documents/topic_en.txt\", top_n=15)\n",
    "    save_top_words(beta_cn, vocab_cn, \"/users/seung-won/documents/topic_cn.txt\", top_n=15)\n",
    "\n",
    "    print(\"Top words saved to 'top_words_en.txt' and 'top_words_cn.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4ec63ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc-topic distributions saved to /users/seung-won/documents/AR_doc_topic_dist_en_20.npy\n",
      "Doc-topic distributions saved to /users/seung-won/documents/AR_doc_topic_dist_cn_20.npy\n"
     ]
    }
   ],
   "source": [
    "def create_dataloader_separate_fixed(x_en, x_cn, batch_size_en, batch_size_cn):\n",
    "    # Convert English data to tensors\n",
    "    x_en_tensor = torch.tensor(x_en, dtype=torch.float32)\n",
    "    dataset_en = TensorDataset(x_en_tensor)\n",
    "    dataloader_en = DataLoader(dataset_en, batch_size=batch_size_en, shuffle=False)\n",
    "\n",
    "    # Convert Chinese data to tensors\n",
    "    x_cn_tensor = torch.tensor(x_cn, dtype=torch.float32)\n",
    "    dataset_cn = TensorDataset(x_cn_tensor)\n",
    "    dataloader_cn = DataLoader(dataset_cn, batch_size=batch_size_cn, shuffle=False)\n",
    "\n",
    "    return dataloader_en, dataloader_cn\n",
    "\n",
    "def save_doc_topic_distributions(model, dataloader, output_file, device):\n",
    "\n",
    "    model.eval()  # 평가 모드로 설정\n",
    "    doc_topic_distributions = []\n",
    "\n",
    "    with torch.no_grad():  # 그래디언트 비활성화\n",
    "        for batch_idx, (x,) in enumerate(dataloader):  # 배치 데이터 로드\n",
    "            x = x.to(device)\n",
    "            # 모델에서 doc-topic 분포 예측\n",
    "            mu = model.get_latent_vector(x)\n",
    "            theta = model.get_theta(mu)  # 모델의 get_theta 함수 호출\n",
    "            doc_topic_distributions.append(theta.cpu().numpy())\n",
    "\n",
    "    # 모든 배치를 하나로 합치기\n",
    "    doc_topic_distributions = np.concatenate(doc_topic_distributions, axis=0)\n",
    "\n",
    "    # .npy 파일로 저장\n",
    "    np.save(output_file, doc_topic_distributions)\n",
    "    print(f\"Doc-topic distributions saved to {output_file}\")\n",
    "\n",
    "    \n",
    "# 사용 예시\n",
    "# 학습된 모델과 DataLoader 준비\n",
    "device = 'cpu'\n",
    "output_file_en = \"/users/seung-won/documents/AR_doc_topic_dist_en_20.npy\"\n",
    "output_file_cn = \"/users/seung-won/documents/AR_doc_topic_dist_cn_20.npy\"\n",
    "\n",
    "\n",
    "# doc-topic 분포 저장\n",
    "dataloader_en, dataloader_cn= create_dataloader_separate_fixed(sbert_doc_embeddings_en, sbert_doc_embeddings_cn,\n",
    "                                                               batch_size_en=128, batch_size_cn=128)\n",
    "save_doc_topic_distributions(model, dataloader_en, output_file_en, device)\n",
    "save_doc_topic_distributions(model, dataloader_cn, output_file_cn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f7c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499945f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92960420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf19b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
